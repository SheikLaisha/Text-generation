{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Generation",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOGvqGVcplFtDC+o25q7bYI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SheikLaisha/Text-generation/blob/main/text/Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmrBD6yLCh6e",
        "outputId": "6ac7dda0-5937-4960-d49c-41a855ef3a4f"
      },
      "source": [
        "# importing dependencies\n",
        "import numpy\n",
        "import sys\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4ciIBRkJ0xQ"
      },
      "source": [
        "# loading the data\n",
        "file = open(\"the_adventures_of_sherlock_holmes_1661-0.txt\").read()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMVE5mliKVqI"
      },
      "source": [
        "# tokenization\n",
        "# standardization\n",
        "# Tokenization - It is the process of breaking a stream of text up into word phrases symbols or other meaningful elements \n",
        "def tokenize_words(input):\n",
        "  #lowercase everything to standardize it\n",
        "  input = input.lower()\n",
        "  # instantiating the tokenizer\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  # tokenizing the text into tokens\n",
        "  tokens = tokenizer.tokenize(input)\n",
        "  # filtering the stopwords using lambda\n",
        "  filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
        "  return \"\".join(filtered)\n",
        "  \n",
        "# preprocess the input data , make tokens  \n",
        "processed_inputs = tokenize_words(file)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0VGNxkzlmCh"
      },
      "source": [
        "# chars to numbers\n",
        "# convert characters in our input to numbers\n",
        "# sort the list of the set of all characters that appear in our i/p text and then use the enumerate fn to get numbers that represent the characters\n",
        "# then create a dictionary that stores the key and values, or the characters and numbers that represent them\n",
        "chars = sorted(list(set(processed_inputs)))\n",
        "char_to_num = dict((c,i) for i, c in enumerate(chars))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EURjhKMw_e9a",
        "outputId": "5385c4da-2150-4220-aa5c-01e39a88a7b5"
      },
      "source": [
        "# check if words to chars or chars to num (?!) has worked?\n",
        "# just so that we get an idea of wheather our process of converting words to characters has worked\n",
        "# print the length of variables \n",
        "input_len = len(processed_inputs)\n",
        "vocab_len = len(chars)\n",
        "print (\"Total number of characters:\", input_len)\n",
        "print (\"Total vocab:\", vocab_len)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters: 281313\n",
            "Total vocab: 44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM0hPEWKBIf5"
      },
      "source": [
        "# sequence length\n",
        "# defining how long we want an individual sequence here\n",
        "# an individual sequence is a completing mapping of input characters as integers\n",
        "seq_length = 100\n",
        "x_data = []\n",
        "y_data = []"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkTUcjYvBWiI",
        "outputId": "506ac4b7-0e17-4602-d0ae-60c9b6c47c10"
      },
      "source": [
        "# loop through the sequence\n",
        "# here,we are going through the entire list of inputs and converting the chars to numbers with for loop\n",
        "# this will create a bunch of sequences where each sequence starts with the next character in the input data \n",
        "# beginning with the first character\n",
        "for i in range(0, input_len - seq_length, 1):\n",
        "  # define input and output sequences\n",
        "  # input is the current character plus desired sequence length\n",
        "  in_seq = processed_inputs[i:i + seq_length]\n",
        "  # out sequence is the initial character plus total sequence length\n",
        "  out_seq = processed_inputs[i + seq_length]\n",
        "  # converting the list of character to integers based on previous values and appending the values to our list\n",
        "  x_data.append([char_to_num[char] for char in in_seq])\n",
        "  y_data.append(char_to_num[out_seq])\n",
        "\n",
        "# check to see how many total input sequence we have \n",
        "n_patterns = len(x_data)\n",
        "print (\"Total Patterns:\", n_patterns)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Patterns: 281213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HBTX2gJDCok"
      },
      "source": [
        "# convert input sequence to np array and so that our network can use\n",
        "x = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n",
        "x = x/float(vocab_len)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLdq-o5lDrpM"
      },
      "source": [
        "# one-hot encoding our label data\n",
        "y = np_utils.to_categorical(y_data)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUhI90I0D5-f"
      },
      "source": [
        "# creating the model \n",
        "# creating a sequential model\n",
        "# dropout is used to prevent overfitting\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(x.shape[1], x.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Id16jjfHwas"
      },
      "source": [
        "# compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVkw3XS_In6d"
      },
      "source": [
        "# saving weights\n",
        "filepath = \"model_weights_saved.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose = 1, save_best_only=True, mode='min')\n",
        "desired_callbacks = [checkpoint]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxZpXPJ2JjMn",
        "outputId": "c15a20b5-bc62-4e1e-aa05-7dacb6b2d427"
      },
      "source": [
        "# fit model and let it train\n",
        "model.fit(x,y, epochs=4, batch_size=256, callbacks=desired_callbacks) "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "1099/1099 [==============================] - 4153s 4s/step - loss: 2.9604\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.96044, saving model to model_weights_saved.hdf5\n",
            "Epoch 2/4\n",
            "1099/1099 [==============================] - 4142s 4s/step - loss: 2.9424\n",
            "\n",
            "Epoch 00002: loss improved from 2.96044 to 2.94245, saving model to model_weights_saved.hdf5\n",
            "Epoch 3/4\n",
            "1099/1099 [==============================] - 4181s 4s/step - loss: 2.9220\n",
            "\n",
            "Epoch 00003: loss improved from 2.94245 to 2.92204, saving model to model_weights_saved.hdf5\n",
            "Epoch 4/4\n",
            "1099/1099 [==============================] - 4171s 4s/step - loss: 2.8719\n",
            "\n",
            "Epoch 00004: loss improved from 2.92204 to 2.87187, saving model to model_weights_saved.hdf5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f862734f510>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixeioyEZ8Wzy"
      },
      "source": [
        "# recompile model with the saved weights\n",
        "filename = \"model_weights_saved.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam') "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pzde1au_icW"
      },
      "source": [
        "# output of the model back into characters\n",
        "num_to_char = dict((i,c) for i,c in enumerate(chars))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyvnPDHvAcy8",
        "outputId": "bb625830-5cc0-4ab5-f58d-77a3e046728d"
      },
      "source": [
        "# random seed to help generate\n",
        "start = numpy.random.randint(0, len(x_data) - 1)\n",
        "pattern = x_data[start]\n",
        "print(\"Random Seed:\")\n",
        "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Seed:\n",
            "\" eservedwarmestthankscouldexplaintruestateaffairswithoutbetrayingonecertainlydeservedlittleenoughcons \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9XHRPHDBlEo",
        "outputId": "3e941376-db1d-4a65-9107-fff0930323a3"
      },
      "source": [
        "# generate the test\n",
        "for i in range(100):\n",
        "  x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "  x = x/float(vocab_len)\n",
        "  prediction = model.predict(x, verbose=0)\n",
        "  index = numpy.argmax(prediction)\n",
        "  result = num_to_char[index]\n",
        "  seq_in = [num_to_char[value] for value in pattern]\n",
        "  sys.stdout.write(result)\n",
        "  pattern.append(index)\n",
        "  pattern = pattern[1:len(pattern)]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "erererererererererererererererererererererererererererererererererererererererererererererererererer"
          ]
        }
      ]
    }
  ]
}